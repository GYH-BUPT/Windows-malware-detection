import os
import pickle as pkl
import torch
import json
from sklearn import metrics
import numpy as np
import torch
from torch.utils.data import TensorDataset, DataLoader
from model_zoo.sage import GraphSage
from model_zoo.han_sage import HANSage
from hin.load_data import load_data
from setting import *


# ============================= load pretrained model ===========================================
def load_pretrain_model(model_file):
    """
    load pretrained model
    """
    model_type = os.path.split(model_file)[0].split('/')[-1]
    model_view = os.path.split(model_file)[1].split('_')[-3]
    logger.debug(model_view)
    if model_view == 'han':
        adj_lists, feat_data, labels = load_data(data_dir)
    else:
        adj_lists, feat_data, labels = load_data(data_dir, 'app_{}_app'.format(model_view))
    
    # logger.info("loading dataset.")
    num_class = 2
    embed_dim = 256
    num_sample = 5
    num_layers = 2
    is_cuda = torch.cuda.is_available()

    map_guessmodel = {
        'GraphSage': GraphSage,
        'HANSage': HANSage,
    }
    model_class = map_guessmodel[model_type]
    model = model_class(adj_lists, feat_data, num_class, embed_dim, num_sample, num_layers, is_cuda)
    checkpoint = torch.load(model_file)
    model.load_state_dict(checkpoint['state_dict'])
    model.eval()
    return model, torch.Tensor(feat_data)


def load_data_for_adv(batch_size=1):
    """
    在攻击的时候只使用test里面的样本
    并且只对malware构造对抗样本，而忽略benign
    """
    with open(os.path.join(data_dir, 'label_info', 'split_info.json'), 'r') as f:
        split_info = json.load(f)
    test_ids = split_info['test_ids']
    test_y = split_info['test_y']
    x_ids = torch.LongTensor(test_ids).unsqueeze(1)
    y = torch.LongTensor(test_y).unsqueeze(1)
    dataset = TensorDataset(x_ids, y)
    data_loader = DataLoader(dataset, batch_size=1, shuffle=False)    
    return data_loader


def evl_index_for_adv(r_codes, alg):
    print("{} adv_attack.".format(alg))
    r_codes = np.array(r_codes)
    print(r_codes)
    rc_0 = np.sum(r_codes==0)   # 模型本来就判错，没有实施攻击
    rc_n = np.sum(r_codes==-1)  # 实施了攻击，攻击结果为失败
    rc_p = np.sum(r_codes>0)    # 实施了攻击，攻击结果为成功。 具体数字代表篡改的bit数量
    assert len(r_codes) == rc_0 + rc_n + rc_p
    print("rc_0={}\trc_n={}\trc_p={}".format(rc_0, rc_n, rc_p))
    recall_orig = (rc_n + rc_p) / (rc_0 + rc_n + rc_p)
    recall_adv = rc_n / (rc_0 + rc_n + rc_p)
    foolrate = rc_p / (rc_n + rc_p)
    print("recall_orig={:.4f}\trecall_adv={:.4f}\tfoolrate={:.4f}".format(recall_orig, recall_adv, foolrate))


# ============================ 第一种评价，在第三方提供的对抗样本上测试 ==================================
def load_adv(adv_path, type='fgsm'):
    with open(adv_path, 'rb') as f:
        data = pkl.load(f)
    ids = data['ids']
    adv_x = data['{}_x'.format(type)]
    
    adv_feat = dict()
    assert len(ids) == adv_x.shape[0]
    for i in range(len(ids)):
        adv_feat[ids[i]] = adv_x[i]
    return adv_feat

def criteria(y, pred):
    f1 = metrics.f1_score(y, pred)
    print('f1: ',f1)
    precision = metrics.precision_score(y, pred)
    print('precision:', precision)
    recall = metrics.recall_score(y, pred)
    print('recall:',recall)
    print(("\nConfusion_Matrix:\n{}".format(metrics.confusion_matrix(y, pred))))


# ================================= 第二种评价：直接针对模型生成对抗样本，看foolingrate ===================
def load_ids(split_file, labels):
    with open(split_file, 'r') as f:
        data = json.load(f)
    ids = data['test']
    ids = list(filter(lambda n: labels[n]==1, ids)) # 只需要针对测试集中的malware尝试生成对抗样本
    ids.sort()
    return ids

