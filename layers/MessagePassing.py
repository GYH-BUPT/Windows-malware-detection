import torch
import torch.nn as nn
from torch.nn import init
import torch.nn.functional as F
from setting import logger
from layers.sampler import node_map
from layers.attention import SelfAttentionLayer

import numpy as np


class SageLayer(nn.Module):
    """
    single view
    """
    def __init__(self, in_dim, out_dim, drop_rate=0.4, cuda=False):
        super(SageLayer, self).__init__()
        self.drop_rt = drop_rate
        self.cuda = cuda
        self.weight = nn.Parameter(torch.FloatTensor(out_dim, in_dim * 2))
        init.xavier_uniform_(self.weight)

    def forward(self, nodes, pre_hidden_embs, info_neighs, layer_i=1):
        _, _, unique_nodes = info_neighs
        nodes = [unique_nodes[x] for x in nodes]
        self_feats = pre_hidden_embs[nodes]
        aggregate_feats = self.message(nodes, pre_hidden_embs, info_neighs, layer_i)
        out_emb = self.update(self_feats, aggregate_feats)
        out_emb = F.dropout(out_emb, self.drop_rt)
        return out_emb

    def message(self, nodes, pre_hidden_embs, info_neighs, layer_i=1):
        """
        获取环境向量
        """
        _, samp_neighs, unique_nodes = info_neighs
        assert len(nodes) == len(samp_neighs)
        mask = torch.zeros(len(samp_neighs), len(unique_nodes))
        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]
        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]
        mask[row_indices, column_indices] = 1
        if self.cuda:
            mask = mask.cuda()
        num_neigh = mask.sum(1, keepdim=True)
        #这里后续更改
        mask = mask.div(num_neigh)

        embed_matrix = pre_hidden_embs
        
        ## m_vector是每个节点的环境向量
        m_vector = mask.mm(embed_matrix)
        return m_vector
    
    
    def update(self, self_feats, aggregate_feats):
        """
        根据目标节点和该节点的环境向量更新表示
        """
        combined = torch.cat([self_feats, aggregate_feats], dim=1)
        combined = F.relu(self.weight.mm(combined.t()))
        return combined.t()


class SageLayer_avg(nn.Module):
    """
    不是拼接邻居，而是通过平均来融合邻居，这样会给邻居更多的权重。有利于增加robustness
    """
    def __init__(self, in_dim, out_dim, drop_rate=0.4, cuda=False):
        super(SageLayer_avg, self).__init__()
        self.drop_rt = drop_rate
        self.cuda = cuda
        self.weight = nn.Parameter(torch.FloatTensor(out_dim, in_dim))
        init.xavier_uniform_(self.weight)

    def forward(self, nodes, pre_hidden_embs, info_neighs, layer_i=1):
        _, _, unique_nodes = info_neighs
        nodes = [unique_nodes[x] for x in nodes]
        self_feats = pre_hidden_embs[nodes]
        aggregate_feats = self.message(nodes, pre_hidden_embs, info_neighs, layer_i)
        out_emb = self.update(self_feats, aggregate_feats)
        out_emb = F.dropout(out_emb, self.drop_rt)
        return out_emb

    def message(self, nodes, pre_hidden_embs, info_neighs, layer_i=1):
        """
        获取环境向量
        """
        _, samp_neighs, unique_nodes = info_neighs
        assert len(nodes) == len(samp_neighs)
        mask = torch.zeros(len(samp_neighs), len(unique_nodes))
        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]
        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]
        mask[row_indices, column_indices] = 1
        if self.cuda:
            mask = mask.cuda()
        num_neigh = mask.sum(1, keepdim=True)
        mask = mask.div(num_neigh)

        embed_matrix = pre_hidden_embs
        
        ## m_vector是每个节点的环境向量
        m_vector = mask.mm(embed_matrix)
        return m_vector
    
    
    def update(self, self_feats, aggregate_feats):
        """
        根据目标节点和该节点的环境向量更新表示
        """
        #combined = torch.cat([self_feats, aggregate_feats], dim=1)
        combined = aggregate_feats
        combined = F.relu(self.weight.mm(combined.t()))
        return combined.t()


class HINLayer(nn.Module):
    """
    multiple views
    metapath_attention
    """
    def __init__(self, in_dim, out_dim, cuda=False):
        super(HINLayer, self).__init__()
        self.cuda = cuda

        self.linear = nn.Linear(in_dim, out_dim, bias=None)
        init.xavier_normal_(self.linear.weight)

        self.fuse_metapath_layer = SelfAttentionLayer(feat_dim = out_dim)

        self.weight = nn.Parameter(torch.FloatTensor(out_dim, out_dim * 2))
        init.xavier_uniform_(self.weight)

    def forward(self, nodes, pre_hidden_embs, info_neighs, layer_i=1):
        unique_nodes_list, samp_neighs, unique_nodes = info_neighs
        if layer_i > 1:
            nodes = node_map(nodes, unique_nodes)
        self_feats = pre_hidden_embs[nodes]
        # 获取每条metapath的环境向量
        paths = []
        for view in samp_neighs.keys():
            aggregate_feats = self.message(nodes, pre_hidden_embs, unique_nodes_list, samp_neighs[view], unique_nodes, layer_i)
            paths.append(aggregate_feats)
        paths = torch.cat([path.view(path.shape[0], 1, path.shape[1]) for path in paths], dim=1)
        
        hidden_paths = self.linear(paths)
        hidden_self = self.linear(self_feats)

        # 使用自注意力机制融合多个metapath的环境向量,得到总的环境向量
        fuse_feats = self.fuse_metapath_layer.forward(hidden_self, hidden_paths)
        
        # 自己 + 环境　＝　新的自己
        out_emb = self.update(hidden_self, fuse_feats)
        return out_emb

    def message(self, nodes, pre_hidden_embs, unique_nodes_list, samp_neighs, unique_nodes, layer_i=1):
        """
        从单个view中获取环境向量
        """
        assert len(nodes) == len(samp_neighs)
        mask = torch.zeros(len(samp_neighs), len(unique_nodes))
        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]
        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]
        mask[row_indices, column_indices] = 1
        if self.cuda:
            mask = mask.cuda()
        num_neigh = mask.sum(1, keepdim=True)
        mask = mask.div(num_neigh)
        if layer_i > 1: # 直接用下一层得到的embedding即可
            embed_matrix = pre_hidden_embs
        else: # 最底下的层需要从整个空间中定位init_embedding
            embed_matrix = pre_hidden_embs[unique_nodes_list]
        
        ## m_vector是每个节点的环境向量
        m_vector = mask.mm(embed_matrix)
        return m_vector

    def update(self, self_feats, aggregate_feats):
        """
        根据目标节点和该节点的环境向量更新表示
        """
        combined = torch.cat([self_feats, aggregate_feats], dim=1)
        combined = F.relu(self.weight.mm(combined.t()))
        return combined.t()
