import torch
import torch.nn as nn
from torch.nn import init
import torch.nn.functional as F
from setting import logger
import torchsnooper

class AttentionLayer(nn.Module):
    """
    融合不同元路径视图时候使用的attention. this is co-attention
    """
    def __init__(self, in_features, out_features, drop_rt=0.4):
        super(AttentionLayer, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.drop_rt=0.4

        self.linear = nn.Linear(in_features, out_features)
        init.xavier_normal_(self.linear.weight)

        self.q = nn.Parameter(torch.randn(out_features)) # q的初始化可以优化下

    #@torchsnooper.snoop()
    def forward(self, inputs):
        hidden = self.linear(inputs)
        atten = torch.matmul(hidden, self.q)
        atten_cof = F.softmax(atten, dim=1)
        atten_cof = atten_cof.reshape(atten_cof.shape[0], 1, atten_cof.shape[1])
        fuse_embed = torch.matmul(atten_cof, hidden)
        fuse_embed.squeeze_()
        fuse_embed = F.dropout(fuse_embed, self.drop_rt)

        return fuse_embed




class SelfAttentionLayer(nn.Module):
    """
    使用自注意力融合不同元路径视图得到的环境向量
    """
    def __init__(self, feat_dim, alpha= 0.2, num_path=2):
        super(SelfAttentionLayer, self).__init__()
        self.feat_dim = feat_dim
        self.num_path = num_path
        self.alpha = alpha

        self.a = nn.Parameter(torch.zeros(size=(2*feat_dim, 1)))
        init.xavier_uniform_(self.a.data, gain=1.414)

        self.leakyrelu = nn.LeakyReLU(self.alpha)

    def forward(self, hidden_self, hidden_paths):
        hidden_self = hidden_self.view(hidden_self.shape[0], 1, hidden_self.shape[1]).repeat(1, hidden_paths.shape[1], 1)
        # get attention weight
        atten_input = torch.cat([hidden_self, hidden_paths], dim=-1)
        atten = self.leakyrelu(torch.matmul(atten_input, self.a))
        atten_cof = F.softmax(atten, dim=1)
        atten_cof = atten_cof.reshape(atten_cof.shape[0], 1, atten_cof.shape[1])
        # weighted average
        fuse_embed = torch.matmul(atten_cof, hidden_paths)
        fuse_embed.squeeze_()

        return fuse_embed

    def __repr__(self):
        return self.__class__.__name__ + ' (' + str(self.num_path) + '*'+str(self.feat_dim) + ' -> ' + str(self.feat_dim)+')'

        