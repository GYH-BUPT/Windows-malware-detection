"""
石川老师团队HAN论文里面的attention方式
每个metapath先独立学习表示，然后attention融合所有元路径的表示
"""
import torch
import torch.nn as nn
from torch.nn import init, Linear
from torch.autograd import Variable
import numpy as np
from sklearn import metrics
import time
import random
import os

from layers.MessagePassing import SageLayer
from layers.sampler import Sampler
from layers.attention import AttentionLayer
from setting import logger
from utils.input_data import InputData
from model_zoo.hander import BaseHander
from model_zoo.classifer import Classifer1 as Classifer
from model_zoo.sage import GraphSage


def re_view(tensor):
    """插入一个维度
    [batchsize, dim] --> [batchsize, 1, dim]
    """
    return tensor.view(tensor.shape[0], 1, tensor.shape[1])


class HANSage(nn.Module):
    """
    第一步：首先使用Sage得到每个元路径的嵌入表示、
    第二步：使用attenion融合
    """
    def __init__(self, adj_lists, feat_data, num_classes, embed_dim, num_sample, num_layers, is_cuda=True):
        super(HANSage, self).__init__()
        self.is_cuda = is_cuda
        self.num_sample = num_sample
        self.embed_dim = embed_dim

        self.encoders = []
        for adj_list in adj_lists:
            self.encoders.append(GraphSage(adj_list, feat_data, num_classes, self.embed_dim, num_sample ,num_layers, self.is_cuda, as_view=True))
        for i, meta_encoder in enumerate(self.encoders):
            self.add_module('metaencoder_{}'.format(i), meta_encoder)
        
        self.atten = AttentionLayer(self.embed_dim, self.embed_dim)
        self.clf = Classifer(self.embed_dim, num_classes)
    
    def get_embedding(self, nodes):
        hiddens = [re_view(encoder.get_embedding(nodes)) for encoder in self.encoders]
        multi_embed = torch.cat(hiddens, dim=1)
        fuse_embed = self.atten(multi_embed)
        return fuse_embed
    
    def forward(self, nodes):
        fuse_embed = self.get_embedding(nodes)
        out = self.clf(fuse_embed)
        return out

    def predict(self, node, node_feat=None):
        """
        predict for single example(node) and allow feat modify of the node
        """
        hiddens = [re_view(encoder.predict(node, node_feat)) for encoder in self.encoders]
        multi_embed = torch.cat(hiddens, dim=1)
        fuse_embed = self.atten(multi_embed)
        out = self.clf(fuse_embed)
        return out.unsqueeze(0)


class HANSageHander(BaseHander):
    """
    wrapper for SupervisedGraphSage model
    """
    def __init__(self, num_class, data, args):
        self.num_class = num_class
        self.labels = data['labels']
        self.adj_lists = data['adj_lists']
        self.feat_data = data['feat_data']
        self.num_nodes, self.feat_dim = self.feat_data.shape
        self.split_seed = args.split_seed
        self.is_cuda = args.cuda
        self.view = args.view
        self.num_sample = args.num_sample
        self.embed_dim = args.embed_dim
        self.freeze = args.freeze
        self.inputdata = InputData(self.num_nodes, self.labels, self.adj_lists, args.split_seed, args.label_rate, self.is_cuda)
        self.train_data_loader = self.inputdata.get_train_data_load(batch_size=args.batch_size, shuffle=True)
    
    def build_model(self):
        """定义模型"""
        logger.info("define model.")
        self.num_layers = 2
        self.model = HANSage(self.adj_lists, self.feat_data, self.num_class, self.embed_dim, self.num_sample, self.num_layers, self.is_cuda)
        logger.info('\n{}'.format(self.model))
        if self.is_cuda:
            self.model.cuda()
            for encoder in self.model.encoders:
                encoder.cuda()
        # 参数初始化
        self.custom_init(self.freeze)
        self.optimizer = torch.optim.Adam(filter(lambda param: param.requires_grad, self.model.parameters()), lr=1e-4, weight_decay=1e-5)
        for i in self.model.parameters():
            logger.debug(i)
        logger.debug(self.model.parameters)
        self.loss_func = nn.CrossEntropyLoss()
    
    def custom_init(self, freeze=False):
        """
        用提前训练好的模型参数去初始化每个view的参数
        """
        logger.info("custom initialization. freeze={}".format(freeze))
        from setting import model_path
        import glob


        all_views = ['pe_imports_pe','pe_export_pe','pe_characteristics_pe','pe_dll_characteristics_pe','pe_sections_pe']

        for i in range(len(all_views)):
            file = glob.glob(os.path.join(model_path, 'GraphSage', "*{}*".format(all_views[i])))[0]
            logger.debug(file)
            checkpoint_tpl = torch.load(glob.glob(os.path.join(model_path, 'GraphSage', "*{}*".format(all_views[i])))[0])
            state_dict = checkpoint_tpl['state_dict']
            self.model.encoders[i].load_state_dict(state_dict, strict=False)
            if freeze:
                for param in self.model.encoders[i].parameters():
                    param.requires_grad = False
